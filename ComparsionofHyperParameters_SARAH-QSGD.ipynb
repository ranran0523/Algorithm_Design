{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2 Anaconda","language":"python","name":"python2anaconda"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.15"},"colab":{"name":"ComparsionofHyperParameters_SARAH-QSGD.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Yey-YMS0IaNY","colab_type":"text"},"source":["1. make sure that the random seeds are working -\n","2. run the training for \"5\" epochs and for each set of hyper-parameters run if e.g. 10 times (each time with different random seed) -\n","3. learn how to plot the \"evolution\", i.e. training/testing error vs. epochs for mutliple runs  (use https://seaborn.pydata.org/) -\n","4. multiple lines will corespond to different \"hyper-parameters\" (\"lr\" and maybe \"momentum\" (0, 0.9, 0.99) ) -\n","5. try to also compare with SignSGD with two versions of SignSGD with momentum -"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"q-R1u2zDIaNa","colab_type":"code","colab":{},"outputId":"c572f338-c69c-46d3-90c3-0de48b02d711"},"source":["! nvidia-smi | grep -A 2 -B 2 \"N/A\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["|===============================+======================+======================|\r\n","|   0  Tesla K80           Off  | 00000000:04:00.0 Off |                    0 |\r\n","| N/A   44C    P0    57W / 149W |  11238MiB / 11439MiB |      0%      Default |\r\n","+-------------------------------+----------------------+----------------------+\r\n","|   1  Tesla K80           Off  | 00000000:05:00.0 Off |                    0 |\r\n","| N/A   37C    P0    74W / 149W |   1901MiB / 11439MiB |      8%      Default |\r\n","+-------------------------------+----------------------+----------------------+\r\n","|   2  Tesla K80           Off  | 00000000:83:00.0 Off |                    0 |\r\n","| N/A   43C    P0    58W / 149W |  10529MiB / 11439MiB |      0%      Default |\r\n","+-------------------------------+----------------------+----------------------+\r\n","|   3  Tesla K80           Off  | 00000000:84:00.0 Off |                    0 |\r\n","| N/A   26C    P8    31W / 149W |     11MiB / 11439MiB |      0%      Default |\r\n","+-------------------------------+----------------------+----------------------+\r\n","                                                                               \r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZhMFTfCDIaNe","colab_type":"code","colab":{}},"source":["import os\n","os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"  # which GPU are we using (from 0 to 3)\n","import torch\n","torch.set_num_threads(0)\n","device=\"cuda:3\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"88WOvuUlIaNh","colab_type":"code","colab":{},"outputId":"c1742ce7-cbdf-40b6-d91d-a67c9bade211"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torch.nn.functional as F\n","import numpy as np\n","import copy\n","import pickle\n","from pandas.core.frame import DataFrame\n","from torch.autograd import Variable\n","torch.cuda.is_available()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"nlKG8yfgIaNj","colab_type":"code","colab":{}},"source":["#! rm logs/*"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"oR2hQLj6IaNm","colab_type":"code","colab":{}},"source":["#! ls logs/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UAGzkKI6IaNo","colab_type":"code","colab":{}},"source":["#device=\"cpu\"   \n","def computeErrorForWholeDataset(dataset):\n","    iteration=0\n","    with torch.no_grad():\n","        losses=[]\n","        for i, (images,labels) in enumerate(dataset):\n","            iteration+=1\n","\n","            images =Variable(images).to(device)\n","            labels =Variable(labels).to(device)\n","\n","            outputs=model(images)\n","            loss=loss_function(outputs,labels)\n","            losses.append(loss.item())\n","    return np.mean(losses)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NeeNYYFeIaNr","colab_type":"code","colab":{}},"source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN,self).__init__()\n","        # input_size:28, same_padding=(filter_size-1)/2, 3-1/2=1:padding\n","        self.cnn1=nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n","        # input_size-filter_size +2(padding)/stride + 1 = 28-3+2(1)/1+1=28\n","        self.batchnorm1=nn.BatchNorm2d(8)\n","        # output_channel:8, batch(8)\n","        self.relu=nn.ReLU()\n","        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n","        #input_size=28/2=14\n","        self.cnn2=nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5, stride=1, padding=2)\n","        # same_padding: (5-1)/2=2:padding_size. \n","        self.batchnorm2=nn.BatchNorm2d(32)\n","        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n","        # input_size=14/2=7\n","        # 32x7x7=1568\n","        self.fc1 =nn.Linear(in_features=1568, out_features=600)\n","        self.dropout= nn.Dropout(p=0.5)\n","        self.fc2 =nn.Linear(in_features=600, out_features=10)\n","    def forward(self,x):\n","        x =self.cnn1(x)\n","        x =self.batchnorm1(x)\n","        x =self.relu(x)\n","        x =self.maxpool1(x)\n","        x =self.cnn2(x)\n","        x =self.batchnorm2(x)\n","        x =self.relu(x)\n","        x =self.maxpool2(x)\n","        x =x.view(-1,1568)\n","        x =self.fc1(x)\n","        x =self.relu(x)\n","        x =self.dropout(x)\n","        x =self.fc2(x)\n","        return x\n","    \n","    def partial_grad(self, data, labels, loss_function):\n","\n","        outputs = self.forward(data)\n","        loss = loss_function(outputs, labels)\n","        #set grad to 0\n","        loss.backward() #compute grad\n","        return loss\n","    \n","    def calculate_loss_grad(self, dataset, loss_function, n_samples):\n","\n","        total_loss = 0.0\n","        full_grad = 0.0\n","        for i_grad, data_grad in enumerate(dataset):\n","            images, labels = data_grad\n","            images, labels = Variable(images).to(device), Variable(labels).to(device) #wrap data and target into variable\n","            total_loss += (1./n_samples) * self.partial_grad(images, labels, loss_function)\n","        \n","        \n","        full_grad =np.sum([(para.grad.data.norm(2)**2).item() for para in self.parameters()]) \n","        \n","        return total_loss, (1./n_samples) * np.sqrt(full_grad)\n","    \n","    def sarah_backward(self, dataset, loss_function, n_epoch, learning_rate):\n","\n","        total_loss_epoch = [0 for i in range(n_epoch)]\n","        grad_norm_epoch = [0 for i in range(n_epoch)]\n","        for epoch in range(n_epoch):\n","            for i, (images,labels) in enumerate(train_load):  \n","                images =Variable(images).to(device)\n","                labels =Variable(labels).to(device)\n","                    \n","            running_loss = 0.0\n","            previous_net_sgd = copy.deepcopy(self) #update previous_net_sgd\n","            previous_net_grad = copy.deepcopy(self) #update previous_net_grad\n","\n","            #Compute full grad\n","            previous_net_grad.zero_grad() # grad = 0\n","            total_loss_epoch[epoch], grad_norm_epoch[epoch] = previous_net_grad.calculate_loss_grad(dataset, loss_function, n_samples)\n","            print(total_loss_epoch[epoch], grad_norm_epoch[epoch])\n","            #Run over the dataset\n","            for i_data, data in enumerate(dataset):\n","                images, labels = data\n","                images, labels = Variable(images).to(device), Variable(labels).to(device) #wrap data and target into variable\n","                \n","                #Compute prev stoc grad\n","                previous_net_sgd.zero_grad() #grad = 0\n","                prev_loss = previous_net_sgd.partial_grad(images, labels, loss_function)\n","                \n","                #Compute cur stoc grad\n","                self.zero_grad() #grad = 0\n","                cur_loss = self.partial_grad(images, labels, loss_function)\n","                \n","                #Backward\n","                for param1, param2, param3 in zip(self.parameters(), previous_net_sgd.parameters(), previous_net_grad.parameters()): \n","                    param1.data -= (learning_rate) * (param1.grad.data - param2.grad.data + (1./n_samples) * param3.grad.data)\n","\n","                # print statistics\n","                running_loss += cur_loss.item()\n","                if i_data % 2500 == 2499:    # print every 2500 mini-batches\n","                    print('[%d, %5d] loss: %.3f' %\n","                        (epoch + 1, i_data + 1, running_loss / 2500))\n","                    running_loss = 0.0\n","                \n","        return total_loss_epoch, grad_norm_epoch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J8XhL74kIaNu","colab_type":"code","colab":{},"outputId":"28740b72-278c-44df-a09f-d29a2e9b9b6e"},"source":["#transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5], [0.5])])\n","train_dataset= datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n","test_dataset= datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True )\n","train_load=torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1000, shuffle=True)\n","test_load=torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n","\n","n_epoch = 5\n","learning_rate = 0.01\n","\n","n_samples = len(train_load)\n","#print(n_samples)\n","\n","model=CNN()\n","CUDA=torch.cuda.is_available()\n","if CUDA:\n","    model=model.cuda()\n","criterion=nn.CrossEntropyLoss()\n","total_loss_epoch, grad_norm_epoch =model.sarah_backward(train_load,criterion, n_epoch, learning_rate)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(tensor(2.3199, device='cuda:0', grad_fn=<AddBackward0>), 1.1520811483379685)\n","(tensor(1.5937, device='cuda:0', grad_fn=<AddBackward0>), 1.1434890029119895)\n","(tensor(0.9384, device='cuda:0', grad_fn=<AddBackward0>), 0.899508545905635)\n","(tensor(0.6061, device='cuda:0', grad_fn=<AddBackward0>), 0.605894322740242)\n","(tensor(0.4504, device='cuda:0', grad_fn=<AddBackward0>), 0.4317017899777938)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ct7X0ndJIaNw","colab_type":"text"},"source":["The following will kill the kernel and will free memory on GPU for others to use"]},{"cell_type":"code","metadata":{"id":"8TrK72r4IaNx","colab_type":"code","colab":{}},"source":["exit(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_BAaniaFIaN0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Mrkd1HoIaN2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j3qssZq0IaN4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}