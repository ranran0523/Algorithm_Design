{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. make sure that the random seeds are working -\n",
    "2. run the training for \"5\" epochs and for each set of hyper-parameters run if e.g. 10 times (each time with different random seed) -\n",
    "3. learn how to plot the \"evolution\", i.e. training/testing error vs. epochs for mutliple runs  (use https://seaborn.pydata.org/) -\n",
    "4. multiple lines will corespond to different \"hyper-parameters\" (\"lr\" and maybe \"momentum\" (0, 0.9, 0.99) ) -\n",
    "5. try to also compare with SignSGD with two versions of SignSGD with momentum -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    74W / 149W |   5494MiB / 11439MiB |     52%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           Off  | 00000000:05:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    74W / 149W |   6100MiB / 11439MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla K80           Off  | 00000000:83:00.0 Off |                    0 |\n",
      "| N/A   48C    P0    73W / 149W |   4548MiB / 11439MiB |     27%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla K80           Off  | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    72W / 149W |   8159MiB / 11439MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi | grep -A 2 -B 2 \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"  # which GPU are we using (from 0 to 3)\n",
    "import torch\n",
    "torch.set_num_threads(0)\n",
    "device=\"cuda:3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "from pandas.core.frame import DataFrame\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! rm logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! ls logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device=\"cpu\"   \n",
    "def computeErrorForWholeDataset(dataset):\n",
    "    iteration=0\n",
    "    with torch.no_grad():\n",
    "        losses=[]\n",
    "        for i, (images,labels) in enumerate(dataset):\n",
    "            iteration+=1\n",
    "\n",
    "            images =Variable(images).to(device)\n",
    "            labels =Variable(labels).to(device)\n",
    "\n",
    "            outputs=model(images)\n",
    "            loss=loss_function(outputs,labels)\n",
    "            losses.append(loss.item())\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        # input_size:28, same_padding=(filter_size-1)/2, 3-1/2=1:padding\n",
    "        self.cnn1=nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        # input_size-filter_size +2(padding)/stride + 1 = 28-3+2(1)/1+1=28\n",
    "        self.batchnorm1=nn.BatchNorm2d(8)\n",
    "        # output_channel:8, batch(8)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "        #input_size=28/2=14\n",
    "        self.cnn2=nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        # same_padding: (5-1)/2=2:padding_size. \n",
    "        self.batchnorm2=nn.BatchNorm2d(32)\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "        # input_size=14/2=7\n",
    "        # 32x7x7=1568\n",
    "        self.fc1 =nn.Linear(in_features=1568, out_features=600)\n",
    "        self.dropout= nn.Dropout(p=0.5)\n",
    "        self.fc2 =nn.Linear(in_features=600, out_features=10)\n",
    "    def forward(self,x):\n",
    "        x =self.cnn1(x)\n",
    "        x =self.batchnorm1(x)\n",
    "        x =self.relu(x)\n",
    "        x =self.maxpool1(x)\n",
    "        x =self.cnn2(x)\n",
    "        x =self.batchnorm2(x)\n",
    "        x =self.relu(x)\n",
    "        x =self.maxpool2(x)\n",
    "        x =x.view(-1,1568)\n",
    "        x =self.fc1(x)\n",
    "        x =self.relu(x)\n",
    "        x =self.dropout(x)\n",
    "        x =self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def partial_grad(self, data, labels, loss_function):\n",
    "\n",
    "        outputs = self.forward(data)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        #set grad to 0\n",
    "        loss.backward() #compute grad\n",
    "        return loss\n",
    "    \n",
    "    def calculate_loss_grad(self, dataset, loss_function, n_samples):\n",
    "\n",
    "        total_loss = 0.0\n",
    "        full_grad = 0.0\n",
    "        for i_grad, data_grad in enumerate(dataset):\n",
    "            images, labels = data_grad\n",
    "            images, labels = Variable(images).to(device), Variable(labels).to(device) #wrap data and target into variable\n",
    "            total_loss += (1./n_samples) * self.partial_grad(images, labels, loss_function)\n",
    "        \n",
    "        \n",
    "        full_grad =np.sum([(para.grad.data.norm(2)**2).item() for para in self.parameters()]) \n",
    "        \n",
    "        return total_loss, (1./n_samples) * np.sqrt(full_grad)\n",
    "    \n",
    "    def sarah_backward(self, dataset, loss_function, n_epoch, learning_rate):\n",
    "\n",
    "        total_loss_epoch = [0 for i in range(n_epoch)]\n",
    "        grad_norm_epoch = [0 for i in range(n_epoch)]\n",
    "        \n",
    "        for epoch in range(n_epoch):\n",
    "            for i, (images,labels) in enumerate(train_load):  \n",
    "                images =Variable(images).to(device)\n",
    "                labels =Variable(labels).to(device)\n",
    "                    \n",
    "            running_loss = 0.0\n",
    "            previous_net_sgd = copy.deepcopy(self) #update previous_net_sgd\n",
    "            previous_net_grad = copy.deepcopy(self) #update previous_net_grad\n",
    "\n",
    "            #Compute full grad\n",
    "            previous_net_grad.zero_grad() # grad = 0\n",
    "            total_loss_epoch[epoch], grad_norm_epoch[epoch] = previous_net_grad.calculate_loss_grad(dataset, loss_function, n_samples)\n",
    "            print(total_loss_epoch[epoch], grad_norm_epoch[epoch])\n",
    "            #Run over the dataset\n",
    "            for i_data, data in enumerate(dataset):\n",
    "                images, labels = data\n",
    "                images, labels = Variable(images).to(device), Variable(labels).to(device) #wrap data and target into variable\n",
    "                \n",
    "                #Compute prev stoc grad\n",
    "                previous_net_sgd.zero_grad() #grad = 0\n",
    "                prev_loss = previous_net_sgd.partial_grad(images, labels, loss_function)\n",
    "                \n",
    "                #Compute cur stoc grad\n",
    "                self.zero_grad() #grad = 0\n",
    "                cur_loss = self.partial_grad(images, labels, loss_function)\n",
    "                \n",
    "                #Backward\n",
    "                for param1, param2, param3 in zip(self.parameters(), previous_net_sgd.parameters(), previous_net_grad.parameters()): \n",
    "                    param1.data -= (learning_rate) * (param1.grad.data - param2.grad.data + (1./n_samples) * param3.grad.data)\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += cur_loss.item() \n",
    "                \n",
    "                if i_data % 2500 == 2499:    # print every 2500 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                        (epoch + 1, i_data + 1, running_loss / 2500))\n",
    "                   # running_loss=[]\n",
    "                    running_loss = 0.0\n",
    "                    \n",
    "                pickle.dump(loss_list,open(logfile,'wb'))\n",
    "                \n",
    "        return total_loss_epoch, grad_norm_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(2.3199, device='cuda:0', grad_fn=<AddBackward0>), 1.1520811483379685)\n",
      "(tensor(1.5937, device='cuda:0', grad_fn=<AddBackward0>), 1.1434890029119895)\n",
      "(tensor(0.9384, device='cuda:0', grad_fn=<AddBackward0>), 0.899508545905635)\n",
      "(tensor(0.6061, device='cuda:0', grad_fn=<AddBackward0>), 0.605894322740242)\n",
      "(tensor(0.4504, device='cuda:0', grad_fn=<AddBackward0>), 0.4317017899777938)\n"
     ]
    }
   ],
   "source": [
    "#transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_dataset= datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset= datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "        \n",
    "train_load=torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1000, shuffle=True)\n",
    "test_load=torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "n_epoch = 5\n",
    "learning_rate = 0.01\n",
    "n_samples = len(train_load)\n",
    "#print(n_samples)\n",
    "\n",
    "model=CNN()\n",
    "CUDA=torch.cuda.is_available()\n",
    "if CUDA:\n",
    "    model=model.cuda()\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "total_loss_epoch, grad_norm_epoch =model.sarah_backward(train_load,criterion, n_epoch, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for eta, momentum  in L:\n",
    "    for experiment in range(10):\n",
    "        \n",
    "        logfile = \"logs/SARAH_seed_%d.log\"%(experiment)\n",
    "        print logfile\n",
    "        if os.path.exists(logfile):\n",
    "            print  \"skipping...\"\n",
    "            continue\n",
    "        \n",
    "        torch.manual_seed(experiment)\n",
    "        torch.cuda.manual_seed(experiment)\n",
    "        \n",
    "        \n",
    "        train_dataset= datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "        test_dataset= datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "        \n",
    "        train_load=torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1000, shuffle=True)\n",
    "        test_load=torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "\n",
    "        #train_loadLB=torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1000, shuffle=True)\n",
    "        #test_loadLB=torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n",
    "        \n",
    "        n_epoch = 10\n",
    "        learning_rate = 0.1\n",
    "        n_samples = len(train_load)\n",
    "        print(n_samples)\n",
    "\n",
    "        model=CNN()\n",
    "        CUDA=torch.cuda.is_available()\n",
    "        if CUDA:\n",
    "            model=model.cuda()\n",
    "        criterion=nn.CrossEntropyLoss()\n",
    "        total_loss_epoch, grad_norm_epoch =model.sarah_backward(train_load,criterion, n_epoch, learning_rate)\n",
    "        \n",
    "print(\"Finished!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will kill the kernel and will free memory on GPU for others to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 Anaconda",
   "language": "python",
   "name": "python2anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
